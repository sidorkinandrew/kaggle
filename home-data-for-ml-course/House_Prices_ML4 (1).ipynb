{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House_Prices_ML4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6482c000-f3a8-4994-e0b6-908b4352155d",
        "id": "jpJvct0-al2t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fnpNazugeB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/itslek/stack-blend-lrs-xgb-lgb-house-prices-k-v17/comments\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  #\n",
        "from datetime import datetime\n",
        "\n",
        "from scipy.stats import skew  # for some statistics\n",
        "from scipy.special import boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "\n",
        "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "import os\n",
        "\n",
        "train = pd.read_csv('./drive/My Drive/kaggle/input/home-data-for-ml-course/train.csv')\n",
        "test = pd.read_csv('./drive/My Drive/kaggle/input/home-data-for-ml-course/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIGzqnHr-K9D",
        "colab_type": "code",
        "outputId": "846cd0d0-2e1f-4094-812c-26f842482cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Train set size:\", train.shape)\n",
        "print(\"Test set size:\", test.shape)\n",
        "print('START data processing', datetime.now(), )\n",
        "\n",
        "train_ID = train['Id']\n",
        "test_ID = test['Id']\n",
        "# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
        "train.drop(['Id'], axis=1, inplace=True)\n",
        "test.drop(['Id'], axis=1, inplace=True)\n",
        "\n",
        "# Deleting outliers\n",
        "train = train[train.GrLivArea < 4500]\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
        "y = train.SalePrice.reset_index(drop=True)\n",
        "train_features = train.drop(['SalePrice'], axis=1)\n",
        "test_features = test\n",
        "\n",
        "features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
        "print(features.shape)\n",
        "# Some of the non-numeric predictors are stored as numbers; we convert them into strings \n",
        "features['MSSubClass'] = features['MSSubClass'].apply(str)\n",
        "features['YrSold'] = features['YrSold'].astype(str)\n",
        "features['MoSold'] = features['MoSold'].astype(str)\n",
        "\n",
        "features['Functional'] = features['Functional'].fillna('Typ')\n",
        "features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n",
        "features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n",
        "features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n",
        "features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n",
        "features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n",
        "\n",
        "features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n",
        "\n",
        "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
        "    features[col] = features[col].fillna(0)\n",
        "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
        "    features[col] = features[col].fillna('None')\n",
        "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
        "    features[col] = features[col].fillna('None')\n",
        "\n",
        "features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
        "\n",
        "objects = []\n",
        "for i in features.columns:\n",
        "    if features[i].dtype == object:\n",
        "        objects.append(i)\n",
        "\n",
        "features.update(features[objects].fillna('None'))\n",
        "\n",
        "features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# Filling in the rest of the NA's\n",
        "\n",
        "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerics = []\n",
        "for i in features.columns:\n",
        "    if features[i].dtype in numeric_dtypes:\n",
        "        numerics.append(i)\n",
        "features.update(features[numerics].fillna(0))\n",
        "\n",
        "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerics2 = []\n",
        "for i in features.columns:\n",
        "    if features[i].dtype in numeric_dtypes:\n",
        "        numerics2.append(i)\n",
        "\n",
        "skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
        "\n",
        "high_skew = skew_features[skew_features > 0.5]\n",
        "skew_index = high_skew.index\n",
        "\n",
        "for i in skew_index:\n",
        "    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n",
        "\n",
        "features = features.drop(['Utilities', 'Street', 'PoolQC', ], axis=1)\n",
        "\n",
        "features['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\n",
        "features['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n",
        "\n",
        "features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n",
        "                                 features['1stFlrSF'] + features['2ndFlrSF'])\n",
        "\n",
        "features['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n",
        "                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n",
        "\n",
        "features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n",
        "                              features['EnclosedPorch'] + features['ScreenPorch'] +\n",
        "                              features['WoodDeckSF'])\n",
        "\n",
        "# simplified features\n",
        "features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
        "features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
        "features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
        "features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
        "features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "print(features.shape)\n",
        "final_features = pd.get_dummies(features).reset_index(drop=True)\n",
        "print(final_features.shape)\n",
        "\n",
        "X = final_features.iloc[:len(y), :]\n",
        "X_sub = final_features.iloc[len(X):, :]\n",
        "\n",
        "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
        "\n",
        "outliers = [30, 88, 462, 631, 1322]\n",
        "X = X.drop(X.index[outliers])\n",
        "y = y.drop(y.index[outliers])\n",
        "\n",
        "overfit = []\n",
        "for i in X.columns:\n",
        "    counts = X[i].value_counts()\n",
        "    zeros = counts.iloc[0]\n",
        "    if zeros / len(X) * 100 > 99.94:\n",
        "        overfit.append(i)\n",
        "\n",
        "overfit = list(overfit)\n",
        "overfit.append('MSZoning_C (all)')\n",
        "\n",
        "X = X.drop(overfit, axis=1).copy()\n",
        "X_sub = X_sub.drop(overfit, axis=1).copy()\n",
        "\n",
        "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
        "\n",
        "# ################## ML ########################################\n",
        "print('START ML', datetime.now(), )\n",
        "\n",
        "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# rmsle\n",
        "def rmsle(y, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "\n",
        "# build our model scoring function\n",
        "def cv_rmse(model):\n",
        "    rmse = np.sqrt(-cross_val_score(model, X, y,\n",
        "                                    scoring=\"neg_mean_squared_error\",\n",
        "                                    cv=kfolds))\n",
        "    return rmse\n",
        "\n",
        "\n",
        "# setup models    \n",
        "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
        "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
        "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
        "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
        "\n",
        "ridge = make_pipeline(RobustScaler(),\n",
        "                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
        "\n",
        "lasso = make_pipeline(RobustScaler(),\n",
        "                      LassoCV(max_iter=1e7, alphas=alphas2,\n",
        "                              random_state=42, cv=kfolds))\n",
        "\n",
        "elasticnet = make_pipeline(RobustScaler(),\n",
        "                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n",
        "                                        cv=kfolds, l1_ratio=e_l1ratio))\n",
        "\n",
        "svr = make_pipeline(RobustScaler(),\n",
        "                    SVR(C=20, epsilon=0.008, gamma=0.0003, ))\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
        "                                max_depth=4, max_features='sqrt',\n",
        "                                min_samples_leaf=15, min_samples_split=10,\n",
        "                                loss='huber', random_state=42)\n",
        "\n",
        "lightgbm = LGBMRegressor(objective='regression',\n",
        "                         num_leaves=4,\n",
        "                         learning_rate=0.01,\n",
        "                         n_estimators=5000,\n",
        "                         max_bin=200,\n",
        "                         bagging_fraction=0.75,\n",
        "                         bagging_freq=5,\n",
        "                         bagging_seed=7,\n",
        "                         feature_fraction=0.2,\n",
        "                         feature_fraction_seed=7,\n",
        "                         verbose=-1,\n",
        "                         # min_data_in_leaf=2,\n",
        "                         # min_sum_hessian_in_leaf=11\n",
        "                         )\n",
        "\n",
        "xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n",
        "                       max_depth=3, min_child_weight=0,\n",
        "                       gamma=0, subsample=0.7,\n",
        "                       colsample_bytree=0.7,\n",
        "                       objective='reg:linear', nthread=-1,\n",
        "                       scale_pos_weight=1, seed=27,\n",
        "                       reg_alpha=0.00006, tree_method = 'gpu_hist')\n",
        "\n",
        "# stack\n",
        "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n",
        "                                            gbr, xgboost, lightgbm),\n",
        "                                meta_regressor=xgboost,\n",
        "                                use_features_in_secondary=True)\n",
        "\n",
        "print('TEST score on CV')\n",
        "\n",
        "score = cv_rmse(ridge)\n",
        "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
        "\n",
        "score = cv_rmse(lasso)\n",
        "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
        "\n",
        "score = cv_rmse(elasticnet)\n",
        "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
        "\n",
        "score = cv_rmse(svr)\n",
        "print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
        "\n",
        "score = cv_rmse(gbr)\n",
        "print(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
        "\n",
        "score = cv_rmse(lightgbm)\n",
        "print(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
        "\n",
        "score = cv_rmse(xgboost)\n",
        "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
        "\n",
        "print('START Fit')\n",
        "print(datetime.now(), 'StackingCVRegressor')\n",
        "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n",
        "print(datetime.now(), 'elasticnet')\n",
        "elastic_model_full_data = elasticnet.fit(X, y)\n",
        "print(datetime.now(), 'lasso')\n",
        "lasso_model_full_data = lasso.fit(X, y)\n",
        "print(datetime.now(), 'ridge')\n",
        "ridge_model_full_data = ridge.fit(X, y)\n",
        "print(datetime.now(), 'svr')\n",
        "svr_model_full_data = svr.fit(X, y)\n",
        "print(datetime.now(), 'GradientBoosting')\n",
        "gbr_model_full_data = gbr.fit(X, y)\n",
        "print(datetime.now(), 'xgboost')\n",
        "xgb_model_full_data = xgboost.fit(X, y)\n",
        "print(datetime.now(), 'lightgbm')\n",
        "lgb_model_full_data = lightgbm.fit(X, y)\n",
        "\n",
        "\n",
        "def blend_models_predict(X=X):\n",
        "    return ((0.1* elastic_model_full_data.predict(X)) + \n",
        "            (0.1 * lasso_model_full_data.predict(X)) + \n",
        "            (0.05 * ridge_model_full_data.predict(X)) + \n",
        "            (0.1 * svr_model_full_data.predict(X)) + \n",
        "            (0.1 * gbr_model_full_data.predict(X)) + \n",
        "            (0.15 * xgb_model_full_data.predict(X)) + \n",
        "            (0.1 * lgb_model_full_data.predict(X)) + \n",
        "            (0.3 * stack_gen_model.predict(np.array(X))))\n",
        "\n",
        "\n",
        "print('RMSLE score on train data:')\n",
        "print(rmsle(y, blend_models_predict(X)))\n",
        "print('MSE score on train data:')\n",
        "print(mean_squared_error(y, blend_models_predict(X)))\n",
        "print('MAE score on train data:')\n",
        "print(mean_absolute_error(np.expm1(y), np.floor(np.expm1(blend_models_predict(X)))))\n",
        "\n",
        "\n",
        "print('Predict submission', datetime.now(), )\n",
        "submission = pd.read_csv(\"./drive/My Drive/kaggle/input/home-data-for-ml-course/sample_submission.csv\")\n",
        "\n",
        "submission.iloc[:, 1] = np.floor(np.expm1(blend_models_predict(X_sub)))\n",
        "\n",
        "submission.to_csv(\"./drive/My Drive/kaggle/input/home-data-for-ml-course/House_price_submission_v17.csv\", index=False)\n",
        "print('Save submission', datetime.now(), )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set size: (1460, 81)\n",
            "Test set size: (1459, 80)\n",
            "START data processing 2020-04-16 19:46:16.695132\n",
            "(2917, 79)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
            "  warnings.warn(PearsonRConstantInputWarning())\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:3538: PearsonRNearConstantInputWarning: An input array is nearly constant; the computed correlation coefficent may be inaccurate.\n",
            "  warnings.warn(PearsonRNearConstantInputWarning())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2917, 86)\n",
            "(2917, 333)\n",
            "X (1458, 333) y (1458,) X_sub (1459, 333)\n",
            "X (1453, 331) y (1453,) X_sub (1459, 331)\n",
            "START ML 2020-04-16 19:46:17.389418\n",
            "TEST score on CV\n",
            "Kernel Ridge score: 0.1024 (0.0143)\n",
            " 2020-04-16 19:46:31.858338\n",
            "Lasso score: 0.1031 (0.0147)\n",
            " 2020-04-16 19:46:44.328131\n",
            "ElasticNet score: 0.1031 (0.0149)\n",
            " 2020-04-16 19:47:34.153047\n",
            "SVR score: 0.1023 (0.0133)\n",
            " 2020-04-16 19:47:45.114650\n",
            "GradientBoosting score: 0.1073 (0.0137)\n",
            " 2020-04-16 19:49:06.030651\n",
            "Lightgbm score: 0.1065 (0.0151)\n",
            " 2020-04-16 19:49:19.352251\n",
            "[19:49:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:49:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:50:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:50:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:50:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:51:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:51:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:51:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:52:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:52:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Xgboost score: 0.1068 (0.0161)\n",
            " 2020-04-16 19:52:40.531860\n",
            "START Fit\n",
            "2020-04-16 19:52:40.532109 StackingCVRegressor\n",
            "[19:54:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:54:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:55:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:55:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[19:56:28] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "2020-04-16 19:56:50.446160 elasticnet\n",
            "2020-04-16 19:56:56.546995 lasso\n",
            "2020-04-16 19:56:57.950237 ridge\n",
            "2020-04-16 19:56:59.677896 svr\n",
            "2020-04-16 19:57:00.972943 GradientBoosting\n",
            "2020-04-16 19:57:09.922805 xgboost\n",
            "[19:57:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "2020-04-16 19:57:30.101497 lightgbm\n",
            "RMSLE score on train data:\n",
            "0.05558176745973037\n",
            "MSE score on train data:\n",
            "0.0030893328739475417\n",
            "MAE score on train data:\n",
            "6825.660701995876\n",
            "Predict submission 2020-04-16 19:57:37.868261\n",
            "Save submission 2020-04-16 19:57:40.366044\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}