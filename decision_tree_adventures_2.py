# -*- coding: utf-8 -*-
"""Decision Tree Adventures 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ifexP1ZvpLLlLHDqEy7bE83fp9lA6MBJ
"""

from google.colab import drive
drive.mount('/content/drive')

# https://medium.com/datadriveninvestor/decision-tree-adventures-2-explanation-of-decision-tree-classifier-parameters-84776f39a28
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, mean_absolute_error
from sklearn import tree
import graphviz 
import numpy as np
from sklearn.externals.six import StringIO  
from IPython.display import Image  
from sklearn.tree import export_graphviz
import pydotplus
from IPython.display import SVG
from graphviz import Source
from IPython.display import display

dataset_url = 'https://raw.githubusercontent.com/haydarozler/decision_tree_adventures/master/StudentsPerformance.csv'
dataset = pd.read_csv(dataset_url)
dataset.head()

dataset["math grade"] = ""
dataset.loc[(dataset["math score"] >= 60), "math grade"] = "Pass"
dataset.loc[(dataset["math score"] < 60), "math grade"] = "Fail"
dataset.drop(columns=['math score', 'reading score', 'writing score'], inplace=True)
one_hot = pd.get_dummies(dataset['gender'], prefix='gender', drop_first=True)
dataset = dataset.join(one_hot)
one_hot = pd.get_dummies(dataset['race/ethnicity'], prefix='race/ethnicity', drop_first=True)
dataset = dataset.join(one_hot)
one_hot = pd.get_dummies(dataset['parental level of education'], prefix='parental level of education', drop_first=True)
dataset = dataset.join(one_hot)
one_hot = pd.get_dummies(dataset['lunch'], prefix='lunch', drop_first=True)
dataset = dataset.join(one_hot)
one_hot = pd.get_dummies(dataset['test preparation course'], prefix='test preparation course', drop_first=True)
dataset = dataset.join(one_hot)
data_train, data_test_hold = train_test_split(dataset, test_size=0.30, random_state=21)
data_test, data_hold = train_test_split(data_test_hold, test_size=0.33, random_state=21)
columns_move = ["gender", "race/ethnicity", "parental level of education", "lunch", "test preparation course", "gender_male", "race/ethnicity_group B", "race/ethnicity_group C", "race/ethnicity_group D", "race/ethnicity_group E", "parental level of education_bachelor's degree", "parental level of education_high school", "parental level of education_master's degree", "parental level of education_some college", "parental level of education_some high school", "lunch_standard", "test preparation course_none"]
y_train = data_train["math grade"].values
X_train = data_train[columns_move].values
y_test = data_test["math grade"].values
X_test = data_test[columns_move].values

X_train, X_test

# plain baseline default model

model = DecisionTreeClassifier()
model.fit(X_train[:,5:], y_train)
y_pred = model.predict(X_test[:,5:])
print("Model Accuracy: %.2f" % (accuracy_score(y_test,y_pred)*100), "%")
dot_data = StringIO()
export_graphviz(model, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

def get_parameter_results(parameter_name, parameter_values, y_label = "accuracy", **kwargs):
    model_data = {'best_parameter' : 0,
                  'best_metric' : 0,
                  'worst_parameter' : 0,
                  'worst_metric' : 100}
    results = {}
    for input_parameter in parameter_values:
        params = {parameter_name : input_parameter}
        params.update(kwargs)
        model = DecisionTreeClassifier(**params)
        model.fit(X_train[:,5:], y_train)      # this is dataset_dependant
        y_pred = model.predict(X_test[:,5:])   # this is dataset_dependant
        acc_score = accuracy_score(y_test,y_pred)*100
        # placeholder for other metrics
        results[input_parameter] = {parameter_name : input_parameter , 'accuracy' : acc_score}
        if acc_score > model_data['best_metric']:
            model_data['best_metric'] = acc_score
            model_data['best_parameter'] = input_parameter
            model_data['best_model'] = model
        if acc_score < model_data['worst_metric']:
            model_data['worst_metric'] = acc_score
            model_data['worst_parameter'] = input_parameter
            model_data['worst_model'] = model
    return results, model_data

def plot_graphs(data, parameter_name, y_label = "accuracy"):
    print(pd.DataFrame(data).T)
    print("")
    plt.figure(figsize=(12,6))
    sns.pointplot(x=parameter_name, y=y_label, data=pd.DataFrame(data).T)
    title = 'Model ' + y_label + ' vs ' + parameter_name + ' parameter'
    plt.title(title)
    plt.xticks(rotation= 90)
    plt.grid()

def print_model_results(model_results, parameter_name, y_label = "accuracy", tree_mode_label = 'best_model'):
    print("BEST PERFORMANCE TREE,", parameter_name, "=", model_results['best_parameter'], ", " +  y_label + " = {:.2f}%".format(model_results['best_metric']))
    print("WORST PERFORMANCE TREE,", parameter_name, "=", model_results['worst_parameter'], ", " +  y_label + " = {:.2f}%".format(model_results['worst_metric']))
    graph = Source(tree.export_graphviz(model_results[tree_mode_label], out_file=None, 
                                        class_names=['Negative', 'Positive'], filled=True,rounded=True,
                                        special_characters=True))
    display(SVG(graph.pipe(format='svg')))

# Establish Model-2
# Take the initial model
# Set random_state=21 (it will be the same for all models)
# Set max_depth with different numbers from 1 to 15: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]

c_parameter_name = 'max_depth'
c_parameter_values = range(1,16)

df, model_results = get_parameter_results(c_parameter_name, c_parameter_values)
plot_graphs(df, c_parameter_name, y_label = "accuracy")
print_model_results(model_results, c_parameter_name, y_label = "accuracy")

# Model-3
# Take the initial model
# Set random_state=21
# Set min_samples_split with different numbers from 5 to 400: 
# [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,125,130,135,140,200,250,300,350,400].
c_parameter_name = 'min_samples_split'
c_parameter_values = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,
                       100,105,110,115,120,125,130,135,140,200,250,300,350,400]

df, model_results = get_parameter_results(c_parameter_name, c_parameter_values, random_state=21)
plot_graphs(df, c_parameter_name, y_label = "accuracy")
print_model_results(model_results, c_parameter_name, y_label = "accuracy")

# Model-4
# Take the initial model.
# Set random_state=21
# Set min_samples_leaf with different numbers from 5 to 200: [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,150,200].
c_parameter_name = 'min_samples_leaf'
c_parameter_values = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,150,200]

df, model_results = get_parameter_results(c_parameter_name, c_parameter_values, random_state=21)
plot_graphs(df, c_parameter_name, y_label = "accuracy")
print_model_results(model_results, c_parameter_name, y_label = "accuracy")

# Model-5
# Take the initial model.
# Set random_state=21.
# Set max_leaf_nodes with different numbers from 2 to 20: [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20].

c_parameter_name = 'max_leaf_nodes'
c_parameter_values = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]

df, model_results = get_parameter_results(c_parameter_name, c_parameter_values, random_state=21)
plot_graphs(df, c_parameter_name, y_label = "accuracy")
print_model_results(model_results, c_parameter_name, y_label = "accuracy")

# Model-6
# Take the inital model
# Set random_state=21
# Set min_impurity_decrease=[0.00005,0.0001,0.0002,0.0005,0.001,0.0015,0.002,0.005,0.01]

c_parameter_name = 'min_impurity_decrease'
c_parameter_values = [0.00005,0.0001,0.0002,0.0005,0.001,0.0015,0.002,0.005,0.01]

df, model_results = get_parameter_results(c_parameter_name, c_parameter_values, random_state=21)
plot_graphs(df, c_parameter_name, y_label = "accuracy")
print_model_results(model_results, c_parameter_name, y_label = "accuracy")

# Model-7
# Take the initial model
# Set random_state=21
# Set criterion=’entropy’
# Set min_impurity_decrease=[0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.15,0.2,0.4]

c_parameter_name = 'min_impurity_decrease'
c_parameter_values = [0.0005,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.15,0.2,0.4]

df, model_results = get_parameter_results(c_parameter_name, c_parameter_values, random_state=21, criterion='entropy')
plot_graphs(df, c_parameter_name, y_label = "accuracy")
print_model_results(model_results, c_parameter_name, y_label = "accuracy")

# Model-8
# Take the initial model
# Set random_state=21
# Set criterion=’entropy’
# Set min_samples_leaf=35
# Set min_impurity_decrease=0.005

c_parameter_name = 'min_impurity_decrease'
c_parameter_values = [0.005]

df, model_results = get_parameter_results(c_parameter_name, c_parameter_values, random_state=21, criterion='entropy', min_samples_leaf=35)
plot_graphs(df, c_parameter_name, y_label = "accuracy")
print_model_results(model_results, c_parameter_name, y_label = "accuracy")