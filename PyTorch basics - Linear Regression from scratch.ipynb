{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# PyTorch basics - Linear Regression from scratch\n\n<!-- <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ECHX1s0Kk-o?controls=0\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> -->\n\nTutorial inspired from [FastAI development notebooks](https://github.com/fastai/fastai_v1/tree/master/dev_nb)\n\n## Machine Learning\n\n<img src=\"https://i.imgur.com/oJEQe7k.png\" width=\"500\">\n\n\n## Tensors & Gradients"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import Numpy & PyTorch\nimport numpy as np\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34f006aa7eb4bbc683c39b7059021da900180908"},"cell_type":"markdown","source":"A tensor is a number, vector, matrix or any n-dimensional array."},{"metadata":{"trusted":true,"_uuid":"e22be3f71825128f990e78959fa00d1331d344e4"},"cell_type":"code","source":"# Create tensors.\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad=True)\nb = torch.tensor(5., requires_grad=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cb90767ff9bc2c12b72548b1a430984241d4910"},"cell_type":"code","source":"# Print tensors\nprint(x)\nprint(w)\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66a939ee0ec472705acd3f23654bc3ccea1cc8b4"},"cell_type":"markdown","source":"We can combine tensors with the usual arithmetic operations."},{"metadata":{"trusted":true,"_uuid":"0bd8fdeb252742e3449b7a2f08bcb188645dc9cf"},"cell_type":"code","source":"# Arithmetic operations\ny = w * x + b\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64e0f175c65c3e875c671c40e4a9bf495e30b772"},"cell_type":"markdown","source":"What makes PyTorch special, is that we can automatically compute the derivative of `y` w.r.t. the tensors that have `requires_grad` set to `True` i.e. `w` and `b`."},{"metadata":{"trusted":true,"_uuid":"6c98996f00294f99eb11989b5a9ecdbda31864e1"},"cell_type":"code","source":"# Compute gradients\ny.backward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a62ffb26a76329e511f9f063c4c26cc6a7dc21"},"cell_type":"code","source":"# Display gradients\nprint('dy/dw:', w.grad)\nprint('dy/db:', b.grad)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b65b6bb4d15127b1d51f09abf616cfd29fa48b4"},"cell_type":"markdown","source":"## Problem Statement"},{"metadata":{"_uuid":"c1beecda01bc332596edd193cade30006e3f6cbf"},"cell_type":"markdown","source":"We'll create a model that predicts crop yeilds for apples and oranges (*target variables*) by looking at the average temperature, rainfall and humidity (*input variables or features*) in a region. Here's the training data:\n\n<img src=\"https://i.imgur.com/lBguUV9.png\" width=\"500\" />\n\nIn a **linear regression** model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n\n```\nyeild_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\nyeild_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n```\n\nVisually, it means that the yield of apples is a linear or planar function of the temperature, rainfall & humidity.\n\n<img src=\"https://i.imgur.com/mtkR2lB.png\" width=\"540\" >\n\n\n**Our objective**: Find a suitable set of *weights* and *biases* using the training data, to make accurate predictions."},{"metadata":{"_uuid":"c24b8195c0e9c6e8e13e169d264484f1f9b3b1ae"},"cell_type":"markdown","source":"## Training Data\nThe training data can be represented using 2 matrices (inputs and targets), each with one row per observation and one column per variable."},{"metadata":{"trusted":true,"_uuid":"dfda99005fc6daf3a49ae1cdd427ccac0aa446b1"},"cell_type":"code","source":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], \n                   [91, 88, 64], \n                   [87, 134, 58], \n                   [102, 43, 37], \n                   [69, 96, 70]], dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf56faf74f7e29c9ed7523308718a9ab1acc0667"},"cell_type":"code","source":"# Targets (apples, oranges)\ntargets = np.array([[56, 70], \n                    [81, 101], \n                    [119, 133], \n                    [22, 37], \n                    [103, 119]], dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70d48f83ae4fce7aba7dd78fd58dddc77c598bfd"},"cell_type":"markdown","source":"Before we build a model, we need to convert inputs and targets to PyTorch tensors."},{"metadata":{"trusted":true,"_uuid":"931c1bad8788e607fa100d4338e1b1fe120e2339"},"cell_type":"code","source":"# Convert inputs and targets to tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\nprint(inputs)\nprint(targets)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"652647cd90bd0784ec4dc53472410f7358ee18c9"},"cell_type":"markdown","source":"## Linear Regression Model (from scratch)\n\nThe *weights* and *biases* can also be represented as matrices, initialized with random values. The first row of `w` and the first element of `b` are use to predict the first target variable i.e. yield for apples, and similarly the second for oranges."},{"metadata":{"trusted":true,"_uuid":"6f788ae559355b3f01667be1554a5d2bdcade8db"},"cell_type":"code","source":"# Weights and biases\nw = torch.randn(2, 3, requires_grad=True)\nb = torch.randn(2, requires_grad=True)\nprint(w)\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3579a065997cae41f7f504916b6bc07878ac768c"},"cell_type":"markdown","source":"The *model* is simply a function that performs a matrix multiplication of the input `x` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n\n$$\n\\hspace{2.5cm} X \\hspace{1.1cm} \\times \\hspace{1.2cm} W^T \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n$$\n\n$$\n\\left[ \\begin{array}{cc}\n73 & 67 & 43 \\\\\n91 & 88 & 64 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n69 & 96 & 70\n\\end{array} \\right]\n%\n\\times\n%\n\\left[ \\begin{array}{cc}\nw_{11} & w_{21} \\\\\nw_{12} & w_{22} \\\\\nw_{13} & w_{23}\n\\end{array} \\right]\n%\n+\n%\n\\left[ \\begin{array}{cc}\nb_{1} & b_{2} \\\\\nb_{1} & b_{2} \\\\\n\\vdots & \\vdots \\\\\nb_{1} & b_{2} \\\\\n\\end{array} \\right]\n$$"},{"metadata":{"trusted":true,"_uuid":"b1119f5ae9688a5f31dba438c7f78ca382deb7e3"},"cell_type":"code","source":"# Define the model\ndef model(x):\n    return x @ w.t() + b","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e0a4644cb1c4ed68a3bcf67a8a156341ac7c853"},"cell_type":"markdown","source":"The matrix obtained by passing the input data to the model is a set of predictions for the target variables."},{"metadata":{"trusted":true,"_uuid":"b042a3cf8f16f4c4380cccbac9d0892719c24190"},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\nprint(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5551ef933de7902c8b5a38ae3d8e4795cb244f38"},"cell_type":"code","source":"# Compare with targets\nprint(targets)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c4a9cf2b3c9152f2f832176bce9a87381e2419c"},"cell_type":"markdown","source":"Because we've started with random weights and biases, the model does not a very good job of predicting the target varaibles."},{"metadata":{"_uuid":"edaae7266f5d47c5e970e1438a812f10d8d35fb4"},"cell_type":"markdown","source":"## Loss Function\n\nWe can compare the predictions with the actual targets, using the following method: \n* Calculate the difference between the two matrices (`preds` and `targets`).\n* Square all elements of the difference matrix to remove negative values.\n* Calculate the average of the elements in the resulting matrix.\n\nThe result is a single number, known as the **mean squared error** (MSE)."},{"metadata":{"trusted":true,"_uuid":"dbf5bca8cbf2a3831089b454c70469e3748e9682"},"cell_type":"code","source":"# MSE loss\ndef mse(t1, t2):\n    diff = t1 - t2\n    return torch.sum(diff * diff) / diff.numel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90da6779aad81608c40cdca77c3c04b68a815c11"},"cell_type":"code","source":"# Compute loss\nloss = mse(preds, targets)\nprint(loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ab3acadf389f30430b55c26c7979dcffaa974a5"},"cell_type":"markdown","source":"The resulting number is called the **loss**, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model. "},{"metadata":{"_uuid":"c61acf9c3cff205d769fc52ed3b1b76f5ae66233"},"cell_type":"markdown","source":"## Compute Gradients\n\nWith PyTorch, we can automatically compute the gradient or derivative of the `loss` w.r.t. to the weights and biases, because they have `requires_grad` set to `True`."},{"metadata":{"trusted":true,"_uuid":"ef66710c6ef1944567c4dc033e1ca316f35490ab"},"cell_type":"code","source":"# Compute gradients\nloss.backward()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6504cddcfb4bfb0817bf03ef460f08f3145a9091"},"cell_type":"markdown","source":"The gradients are stored in the `.grad` property of the respective tensors."},{"metadata":{"trusted":true,"_uuid":"5943d1cef604a178c95f5e8d255519d42d9f9982"},"cell_type":"code","source":"# Gradients for weights\nprint(w)\nprint(w.grad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47278e318b156c6a5812e0842dbc4164c8362562"},"cell_type":"code","source":"# Gradients for bias\nprint(b)\nprint(b.grad)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"466dc3a2cc2d4bd2c10ae4cf59cf4627b5cc9c75"},"cell_type":"markdown","source":"A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases. \n\n* If a gradient element is **postive**, \n    * **increasing** the element's value slightly will **increase** the loss.\n    * **decreasing** the element's value slightly will **decrease** the loss.\n\n<img src=\"https://i.imgur.com/2H4INoV.png\" width=\"400\" />\n\n\n\n* If a gradient element is **negative**,\n    * **increasing** the element's value slightly will **decrease** the loss.\n    * **decreasing** the element's value slightly will **increase** the loss.\n    \n<img src=\"https://i.imgur.com/h7E2uAv.png\" width=\"400\" />    \n\nThe increase or decrease is proportional to the value of the gradient."},{"metadata":{"_uuid":"35ed968bfc135bd44eeb100ae401d0628fbc5c63"},"cell_type":"markdown","source":"Finally, we'll reset the gradients to zero before moving forward, because PyTorch accumulates gradients."},{"metadata":{"trusted":true,"_uuid":"5f02dc376c21857d4e545d98413952c5ac73039b"},"cell_type":"code","source":"w.grad.zero_()\nb.grad.zero_()\nprint(w.grad)\nprint(b.grad)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5501c66c9729c4954e9b798a0634a9d84487e639"},"cell_type":"markdown","source":"## Adjust weights and biases using gradient descent\n\nWe'll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps:\n\n1. Generate predictions\n2. Calculate the loss\n3. Compute gradients w.r.t the weights and biases\n4. Adjust the weights by subtracting a small quantity proportional to the gradient\n5. Reset the gradients to zero"},{"metadata":{"trusted":true,"_uuid":"ef0d2bd2d9c5acb60992e238439ee00c2223319f"},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\nprint(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"302ee8226da4ee5d0dad137c638573a79f8abded"},"cell_type":"code","source":"# Calculate the loss\nloss = mse(preds, targets)\nprint(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c596aecf87e4670033ddd4ed36e26b97e2f9ab"},"cell_type":"code","source":"# Compute gradients\nloss.backward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec1e2bdc8f91523e556fad55ee8c01eb5431ae24"},"cell_type":"code","source":"# Adjust weights & reset gradients\nwith torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n    w.grad.zero_()\n    b.grad.zero_()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d61b6f61f49b19099d29d1be8ec5ae4967bbd51"},"cell_type":"code","source":"print(w)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6af10c29db7cb0d6e869b2c30966a34a48a011e2"},"cell_type":"markdown","source":"With the new weights and biases, the model should have a lower loss."},{"metadata":{"trusted":true,"_uuid":"c542b5fe75d82454f34cac13cdcff8b48dd1945c"},"cell_type":"code","source":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5201901695f3ea13d7fdd5d985da7e0761c541d0"},"cell_type":"markdown","source":"## Train for multiple epochs\n\nTo reduce the loss further, we repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch."},{"metadata":{"trusted":true,"_uuid":"9f5f0ffeee666b30c5828636359f0be6addbef7c"},"cell_type":"code","source":"# Train for 100 epochs\nfor i in range(100):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4820ca48b78f4dc242d80a9ec3ec6aca1aef671"},"cell_type":"code","source":"# Calculate loss\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbcd65fa7094cec187565e54c2107e683bea787b"},"cell_type":"code","source":"# Print predictions\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"addec2c4eca8edfcae5544ea2cc717182c21d90f"},"cell_type":"code","source":"# Print targets\ntargets","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecc6e79cdfb6a8ca882895ccc895b61b960b0a04"},"cell_type":"markdown","source":"## Linear Regression Model using PyTorch built-ins\n\nLet's re-implement the same model using some built-in functions and classes from PyTorch."},{"metadata":{"trusted":true,"_uuid":"ce66cf0d09a3f38bf2f00ea40418c56d98f1f814"},"cell_type":"code","source":"# Imports\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74bb18bd01ac809079eeb8d05695206e8ba02069"},"cell_type":"code","source":"# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype='float32')\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d94b355f55250e9c7dcff668920f02d7c5c04925"},"cell_type":"code","source":"inputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0665466eb5401f40a816b323a34450b2c052c41"},"cell_type":"markdown","source":"### Dataset and DataLoader\n\nWe'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples. We'll also create a DataLoader, to split the data into batches while training. It also provides other utilities like shuffling and sampling."},{"metadata":{"trusted":true,"_uuid":"206f5fd0473386476b23477bf38d2c327b6376c9"},"cell_type":"code","source":"# Import tensor dataset & data loader\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c47a4f2f86fda3918094e01cf7ab0698bbb5acc7"},"cell_type":"code","source":"# Define dataset\ntrain_ds = TensorDataset(inputs, targets)\ntrain_ds[0:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a2f69126319d738b82ae67d5d404ecd6161bfac"},"cell_type":"code","source":"# Define data loader\nbatch_size = 5\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)\nnext(iter(train_dl))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"276a262e1b9e3a048bcd32989013f9c501c59037"},"cell_type":"markdown","source":"### nn.Linear\nInstead of initializing the weights & biases manually, we can define the model using `nn.Linear`."},{"metadata":{"trusted":true,"_uuid":"59da3506559a0640d80d18f77b02726a1757be2f"},"cell_type":"code","source":"# Define model\nmodel = nn.Linear(3, 2)\nprint(model.weight)\nprint(model.bias)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3a4a8c499a4680f2533329712de034671dd1cdd"},"cell_type":"markdown","source":"### Optimizer\nInstead of manually manipulating the weights & biases using gradients, we can use the optimizer `optim.SGD`."},{"metadata":{"trusted":true,"_uuid":"1848398bd1ced8c25a7bb55612cf32a774500280"},"cell_type":"code","source":"# Define optimizer\nopt = torch.optim.SGD(model.parameters(), lr=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28cbe62be55010bd11b31d819cff38da5a772b18"},"cell_type":"markdown","source":"### Loss Function\nInstead of defining a loss function manually, we can use the built-in loss function `mse_loss`."},{"metadata":{"trusted":true,"_uuid":"69d7f4e8e27ccd077f711da27f8bede8aa711893"},"cell_type":"code","source":"# Import nn.functional\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a02ff888ed4be720fd9ca376022d8fdcf2559683"},"cell_type":"code","source":"# Define loss function\nloss_fn = F.mse_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a540adf76725ea9968025f6c029fdd251bdada6c"},"cell_type":"code","source":"loss = loss_fn(model(inputs), targets)\nprint(loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e833614a69ff18c554a3d89f643ae2f11e0260f6"},"cell_type":"markdown","source":"### Train the model\n\nWe are ready to train the model now. We can define a utility function `fit` which trains the model for a given number of epochs."},{"metadata":{"trusted":true,"_uuid":"128bc7260221f5338edf8b503c75f0c7d1cce7e8"},"cell_type":"code","source":"# Define a utility function to train the model\ndef fit(num_epochs, model, loss_fn, opt):\n    for epoch in range(num_epochs):\n        for xb,yb in train_dl:\n            # Generate predictions\n            pred = model(xb)\n            loss = loss_fn(pred, yb)\n            # Perform gradient descent\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n    print('Training loss: ', loss_fn(model(inputs), targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae8ca4686cf6a68f6c9ca93bf3d227abe96c2201"},"cell_type":"code","source":"# Train the model for 100 epochs\nfit(100, model, loss_fn, opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32588a47d0478772a1f08fa55874a322630bd0b6"},"cell_type":"code","source":"# Generate predictions\npreds = model(inputs)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12d757c0f37c2e3af65cf9d4b59878cc10c65acf"},"cell_type":"code","source":"# Compare with targets\ntargets","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e182289ebf21d8296f11f13264c4732c100da14f"},"cell_type":"markdown","source":"# Bonus: Feedfoward Neural Network\n\n![ffnn](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Multi-Layer_Neural_Network-Vector-Blank.svg/400px-Multi-Layer_Neural_Network-Vector-Blank.svg.png)\n\nConceptually, you think of feedforward neural networks as two or more linear regression models stacked on top of one another with a non-linear activation function applied between them.\n\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png\" width=\"640\">\n\nTo use a feedforward neural network instead of linear regression, we can extend the `nn.Module` class from PyTorch."},{"metadata":{"trusted":true,"_uuid":"c405e5075d6c4adb26ead75c17be90eaeb43f2d5"},"cell_type":"code","source":"class SimpleNet(nn.Module):\n    # Initialize the layers\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(3, 3)\n        self.act1 = nn.ReLU() # Activation function\n        self.linear2 = nn.Linear(3, 2)\n    \n    # Perform the computation\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.act1(x)\n        x = self.linear2(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2448d9832722f4f2813f8bd80b91daefd901dc2e"},"cell_type":"markdown","source":"Now we can define the model, optimizer and loss function exactly as before."},{"metadata":{"trusted":true,"_uuid":"a51ca222c2ea037c3caccaeab98ccdbcc30800cf"},"cell_type":"code","source":"model = SimpleNet()\nopt = torch.optim.SGD(model.parameters(), 1e-5)\nloss_fn = F.mse_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21000c9739ea39a173a256f87339bfc112c1a9b0"},"cell_type":"markdown","source":"Finally, we can apply gradient descent to train the model using the same `fit` function defined earlier for linear regression.\n\n<img src=\"https://i.imgur.com/g7Rl0r8.png\" width=\"500\">"},{"metadata":{"trusted":true,"_uuid":"e94de6868c76803a998c1c1934ed229c826f3b8c"},"cell_type":"code","source":"fit(100, model, loss_fn, opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1132733442c7e7847ebd4b28d8c53c40ab6289cc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}